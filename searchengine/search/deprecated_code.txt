# ------------------------------------- CZYTAMY PLIK ZE SPOJNIKAMI I PRZYIMKAMI -------------------------------------- #
try:
    temp = list()
    linkingWords = open("linkingWords.txt").read().split()
except FileNotFoundError as e:
    linkingWords = list()
	
try:
    linkingWords = open("linkingWords.txt").read().split()
except FileNotFoundError as e:
    linkingWords = open("linkingWords.txt", "w+").write("")
    print(e)

for row in range(len(linkingWords)):
    linkingWords[row] = linkingWords[row].casefold()
	
for element in range(len(row)):
	if type(row[element]) is not str:
		continue
	for word in linkingWords:
		if word in row[element]:
			row[element] = row[element].replace(" " + word + " ", " ")
# ------------------------------------------------------ KONIEC ------------------------------------------------------ #

# -------------------------------------------------- WORDEMBEDDING --------------------------------------------------- #
jeden wyraz
print(model.wv.most_similar(positive=['girl'], topn=3))  # pokaz najbardziej podobne
print(model.wv.most_similar(negative=['girl'], topn=3))  # pokaz najmniej podobne

j = 1  # iterator - wiersze
for row in dfKey.values:  # for ewery (~R.W.) keyword
    tempPositivities = list()  # tymczasowa lista, przechowuje info o jednym wierszu
    score = model.wv.most_similar(positive=row, topn=3)  # 3 najbardziej podobne wyniki do keyworda
    tempPositivities.append(row[0])  # dodaj keyworda
    for i in range(3):
        tempPositivities.append(score[i][0])  # utnij prawdopodobienstwo, dodaj tylko nazwe

    if j == 1:
       positivities = np.array(tempPositivities)  # pierwszy wpis
    else:
       positivities = np.append(positivities, tempPositivities).reshape((j, len(tempPositivities)))  # konwersja na 2d

    j += 1  # kolejny wiersz

print(positivities)

print(model.wv.vocab)

for word in dfKey.values:
   print(word, " =>", model.wv[word])
# ------------------------------------------------------ KONIEC ------------------------------------------------------ #

# ---------------------------------------------------- KEYWORDS ------------------------------------------------------ #
path_to_datakeywords = "datakeywords.csv"
keywords = list()

# keywords -- START
temp_keywords = pd.read_csv(path_to_datakeywords, sep=";", index_col=0).to_numpy()

for el in range(len(list_of_infos)):
    if type(list_of_infos[el]) is int or type(list_of_infos[el]) is float:
        continue
    for word in list_of_infos[el].split():  # pętla która zapisuje wszystkie keywords (category)
        word = word.casefold()
        if word in temp_keywords or word in arg_keywords:  # jesli keywords juz istnieje
            continue  # pomin
        arg_keywords.append(word)  # jesli nie to dopisz do bazy
# keywords -- END

temp_datakeywords = pd.DataFrame(arg_keywords, columns=['keyword'])
arg_datakeywords = pd.concat([arg_datakeywords, temp_datakeywords])  # to samo co wyżej
arg_datakeywords = arg_datakeywords.reset_index(drop=True)  # to samo co wyżej
arg_datakeywords.to_csv(path_to_datakeywords, sep=";")  # zmiana na plik .csv

try:
    datakeywords = pd.read_csv(path_to_datakeywords, sep=";", index_col=0)  # czytaj plik .csv
except FileNotFoundError as e:
    open(path_to_datakeywords, "w+").write(";keyword")  # jeśli go nie ma, to utwórz
    datakeywords = pd.read_csv(path_to_datakeywords, sep=";", index_col=0)  # a potem czytaj
# ------------------------------------------------------ KONIEC ------------------------------------------------------ #



# ----------------------------------------------- DATASET TO KEYWORDS ------------------------------------------------ #
import numpy as np
import pandas as pd


# region Variables
path_to_dataset = "dataset.csv"
path_to_keywords = "datakeywords.csv"
percentages = [5*p for p in range(1, 21)]
corpus = list()
i = 0
# endregion

# region Files
df_set = pd.read_csv(path_to_dataset, sep=";", index_col=0)  # baza danych (czytamy pliki)
try:
    datakeywords = pd.read_csv(path_to_keywords, sep=";", index_col=0).to_numpy()  # czytaj plik .csv
except FileNotFoundError as e:
    open(path_to_keywords, "w+").write(";keyword")  # jeśli go nie ma, to utwórz
    datakeywords = pd.read_csv(path_to_keywords, sep=";", index_col=0).to_numpy()  # a potem czytaj
# endregion

# region Tokenization -- DEPRECATED
print("Warning! Tokenization is DEPRECATED!");
for row in df_set.values:
    temp_str = ""
    for i in range(len(row)):
        if i == 1 or i == 3:
            continue
        if str(row[i]).casefold() != 'nan':
            temp_str += str(row[i]).replace('.', ',').casefold() + '.'
    corpus.append(temp_str)
corpus = "".join(corpus)
corpus = corpus.replace('?', '.').replace('!', '.').split('.')  # zamiana znaków '?' i '!' na kropki
tokenized_sentences = [sentence.replace('.', '').split() for sentence in corpus]  # wyrazy z sentencji
tokenized_sentences_len = len(tokenized_sentences)
# endregion

# region Keywords
percentage = tokenized_sentences_len / 100
for sentence in tokenized_sentences:
    i += 1
    for word in sentence:
        try:
            int(word)
        except ValueError:
            if word in datakeywords:
                continue
            datakeywords = np.append(datakeywords, word)
    actual_percentage = round(i / percentage)
    for j in percentages:
        if i == int(percentage * j):
            print(actual_percentage, '% [{}/{}]'.format(i, tokenized_sentences_len))

datakeywords = pd.DataFrame(datakeywords, columns=['keyword'])
datakeywords.to_csv(path_to_keywords, sep=";")  # zmiana na plik .csv
# endregion
# ------------------------------------------------------ KONIEC ------------------------------------------------------ #